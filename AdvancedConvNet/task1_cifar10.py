# -*- coding: utf-8 -*-
"""CVLab

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dmXMr3Wrr7q_HEMAljGF8HLdj3Vdol0U
"""

from __future__ import print_function
import keras
from keras.datasets import cifar10
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten, Input
from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D
from keras.layers.normalization import BatchNormalization as BN
from keras.layers import GaussianNoise as GN
from keras.layers.merge import concatenate
from keras.regularizers import l2
from keras.optimizers import SGD, Adam
from keras.callbacks import LearningRateScheduler as LRS
from keras.preprocessing.image import ImageDataGenerator
import functions.py as fn


batch_size = 128
num_classes = 10
epochs = 75

#### LOAD AND TRANSFORM
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

x_train /= 255
x_test /= 255

print(x_train.shape)
print(x_test.shape)

y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

  
datagen = ImageDataGenerator(
  width_shift_range=0.1,
  height_shift_range=0.1,
  horizontal_flip=True)

datagen.fit(x_train)


## DEF NN TOPOLOGY  

# PARAMETERS
# Only change this part to get networks in report. 

k=4 # Use as k in wide residual network
dropout=0.1 # Use for dropout in wide residual network. In report: 0.1 for 3x(16+32+64) and 6x(16+32+64), 0.05 for 9x(16x32x64) and 12x(16+32+64)
growth_rate = 16 # Use for denseNet. Multiply by k for "wide denseNet". 
compression=0.5 # Use for denseNet with compression. 0.5 used in report, but this is much, 0.8 would influence performance less. 
bottleneckDense=True   # Set true to add bottleneck to denseNet
filters=16         # Always 16 in report

runPerBlock = 4 # 3/6/9/12 for all but denseNet, 4/10/16/8 for denseNet
typeConv="denseNet" # "convNet", "resNet", "bottleneck", "wide" or "denseNet"

modelInput = Input(shape=(32, 32, 3))

# Chose activation=False for denseNet bcs of preactivation, True, for the rest.
model=fn.initial_conv(modelInput, 16, activation=True)

# No need to change blocks to get runs in report

# Block 1
model, filters=fn.runConv(model, filters, pos="Start", typeConv=typeConv, growth_rate=growth_rate, k=k, dropout=dropout, bottleneckDense=bottleneckDense, compression=compression)
for i in range(runPerBlock-2): 
  model, filters=fn.runConv(model, filters, pos="Middle", typeConv=typeConv, growth_rate=growth_rate, k=k, dropout=dropout, bottleneckDense=bottleneckDense, compression=compression)
model, filters=fn.runConv(model, filters, pos="Last", typeConv=typeConv, growth_rate=growth_rate, k=k, dropout=dropout, bottleneckDense=bottleneckDense, compression=compression)

# Block 2
model, filters=fn.runConv(model, filters, pos="First", typeConv=typeConv, growth_rate=growth_rate, k=k, dropout=dropout, bottleneckDense=bottleneckDense, compression=compression)
for i in range(runPerBlock-2):
  model, filters=fn.runConv(model, filters, pos="Middle", typeConv=typeConv, growth_rate=growth_rate, k=k, dropout=dropout, bottleneckDense=bottleneckDense, compression=compression)
model, filters=fn.runConv(model, filters, pos="Last", typeConv=typeConv, growth_rate=growth_rate, k=k, dropout=dropout, bottleneckDense=bottleneckDense, compression=compression)

# Block 3
model, filters=fn.runConv(model, filters, pos="First", typeConv=typeConv, growth_rate=growth_rate, k=k, dropout=dropout, bottleneckDense=bottleneckDense, compression=compression)
for i in range(runPerBlock-2):
  model, filters=fn.runConv(model, filters, pos="Middle", typeConv=typeConv, growth_rate=growth_rate, k=k, dropout=dropout, bottleneckDense=bottleneckDense, compression=compression)
model, filters=fn.runConv(model, filters, pos="End", typeConv=typeConv, growth_rate=growth_rate, k=k, dropout=dropout, bottleneckDense=bottleneckDense, compression=compression)

model = BN()(model)
model = Activation('relu')(model) 
model = AveragePooling2D(pool_size=8)(model)

model = Flatten()(model)
model = Dense(num_classes, kernel_initializer = 'he_normal')(model)
model = Activation('softmax')(model)

modelFinal = Model(inputs=[modelInput], outputs=[model])

modelFinal.summary()


## OPTIM AND COMPILE
modelFinal.compile(loss='categorical_crossentropy',
              optimizer=Adam(lr=0.001),
              metrics=['accuracy'])


# DEFINE A LEARNING RATE SCHEDULER
def scheduler(epoch):
    if epoch  < 25:
        return .001
    elif epoch < 50:
        return 0.0001
    else:
        return 0.00001

set_lr = LRS(scheduler)


## TRAINING

history=modelFinal.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                            steps_per_epoch=len(x_train) / batch_size, 
                            epochs=epochs,
                            validation_data=(x_test, y_test),
                            callbacks=[set_lr],
                            verbose=1)

