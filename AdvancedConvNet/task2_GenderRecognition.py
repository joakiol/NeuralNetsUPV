# -*- coding: utf-8 -*-
"""faces

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HRsiZi1kK3KScW7iHrk0eTtmmI3RWkv8
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import matplotlib.pyplot as plt

from __future__ import print_function
import keras
from keras.datasets import cifar10
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten, Input
from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D
from keras.layers.normalization import BatchNormalization as BN
from keras.layers import GaussianNoise as GN
from keras.layers.merge import concatenate
from keras.regularizers import l2
from keras.optimizers import SGD, Adam
from keras.callbacks import LearningRateScheduler as LRS
from keras.preprocessing.image import ImageDataGenerator
import functions as fc

# Load, downloaded from https://www.dropbox.com/s/zcwlujrtz3izcw8/gender.tgz
x_train = np.load('/content/drive/My Drive/UPV/x_train.npy')
x_test = np.load('/content/drive/My Drive/UPV/x_test.npy')

y_train = np.load('/content/drive/My Drive/UPV/y_train.npy')
y_test = np.load('/content/drive/My Drive/UPV/y_test.npy')

## Transforms
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

y_train = y_train.astype('float32')
y_test = y_test.astype('float32')

x_train /= 255
x_test /= 255

print(x_train.shape)
print(x_test.shape)

batch_size = 128
num_classes = 2
epochs = 75

y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)


# Data augmentation
datagen = ImageDataGenerator(
  featurewise_center=True,
  featurewise_std_normalization=True,
  width_shift_range=0.2,
  height_shift_range=0.2,
  horizontal_flip=True)

datagen.fit(x_train)

testdatagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
)

testdatagen.fit(x_train)


## DEF NN TOPOLOGY  
modelInput = Input(shape=(100, 100, 3))

k=2
dropout=0.1 
growth_rate = 6
compression=0.8

model=fc.initial_conv(modelInput, 16, activation=False, strides=(2,2)) # Activation=True for CBN and wideResNet

# Runs the latest configuration of denseNet with gender recognition
model, filters=fc.denseNet(model, 16, growth_rate, bottleneck=True)
for i in range(6):
  model, filters=fc.denseNet(model, filters, growth_rate, bottleneck=True)
model, filters=fc.denseNet(model, filters, growth_rate, pool=True, bottleneck=True, compression=compression)

model, filters=fc.denseNet(model, filters, growth_rate, bottleneck=True)
for i in range(6):
  model, filters=fc.denseNet(model, filters, growth_rate, bottleneck=True)
model, filters=fc.denseNet(model, filters, growth_rate, pool=True, bottleneck=True, compression=compression)

model, filters=fc.denseNet(model, filters, growth_rate, bottleneck=True)
for i in range(6):
  model, filters=fc.denseNet(model, filters, growth_rate, bottleneck=True)
model, filters=fc.denseNet(model, filters, growth_rate, bottleneck=True)

# Runs standard conv net with gender recognition
#model = fc.CBN(model, 32)
#model = fc.CBN(model, 64)
#model = fc.CBN(model, 128)
#model = fc.CBN(model, 256)
#model = fc.CBN(model, 512)

# Runs wideResNet with gender recognition
#model=fc.wideResNet(model, 16, k, dropout=dropout, upscale=True)
#model=fc.wideResNet(model, 16, k, dropout=dropout)
#model=fc.wideResNet(model, 16, k, dropout=dropout)

#model=fc.wideResNet(model, 32, k, dropout=dropout, firstStrides=(2,2))
#model=fc.wideResNet(model, 32, k, dropout=dropout)
#model=fc.wideResNet(model, 32, k, dropout=dropout)

#model=fc.wideResNet(model, 64, k, dropout=dropout, firstStrides=(2,2))
#model=fc.wideResNet(model, 64, k, dropout=dropout)
#model=fc.wideResNet(model, 64, k, dropout=dropout)



model = BN()(model)
model = Activation('relu')(model)

model = AveragePooling2D(pool_size=8)(model) #Remove in CBN-run

model = Flatten()(model)
model = Dense(num_classes, kernel_initializer = 'he_normal')(model)
model = Activation('softmax')(model)

modelFinal = Model(inputs=[modelInput], outputs=[model])

modelFinal.summary()


## OPTIM AND COMPILE
opt = SGD(lr=0.1, decay=1e-6)

modelFinal.compile(loss='categorical_crossentropy',
              optimizer=Adam(lr=0.001), 
              metrics=['accuracy'])


# DEFINE A LEARNING RATE SCHEDULER
def scheduler(epoch):
    if epoch  < 25:
        return .001
    elif epoch < 50:
        return 0.0001
    else:
        return 0.00001

set_lr = LRS(scheduler)


## TRAINING

history=modelFinal.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                            steps_per_epoch=len(x_train) / batch_size, 
                            epochs=epochs,
                            validation_data=testdatagen.flow(x_test, y_test), # uncomment this for simpler data aug
                            validation_steps=len(x_test) / batch_size, # uncomment this for simpler data aug
                            #validation_data=(x_test, y_test), # comment this for simpler data aug
                            callbacks=[set_lr])